Question 1

The goal of this project is to buid an Enron person of interest identifier. I will be using the email and financial data that was made public by the federal investigation into the possibility of corporate fraud in 2002. I will also be using the hand-generated list of persons of interest as well as the starter code that was provided by the mini-projects of the Machine Learning class. I will be using machine learning to accomplish this goal because of ability of the algorithms to quickly go through the data and find patterns based on what they are told to look out for. They take care of the complicated mathmatical equations and will help me choose which features provide the most useful information, depending on what I am are looking for at the time. There were outliers in the original data, which I took care with pop.

Question 2

For features I ended up using poi, salary, bonus, restricted_stock_deferred, deferred_income, total_stock_value, exercised_stock_options, long_term_incentive, and restricted_stock. I used these features because I wanted to see if I could find POIs based soley on their financial features. I tried usinge scaling to see if it would improve the accuracy, precision and/or recall scores, but it acutally ended up reducing those numbers.

While I was exploring the dataset I noticed that some POIs didn't have emails showing up. I was curious about how many POIs didn't have emails that connected them to other POIs. I created a no_emails feature to see how many POIs didn't have any emails. I found four POIs ('KOPPER MICHAEL J', 'FASTOW ANDREW S', 'YEAGER F SCOTT', 'HIRKO JOSEPH') that didn't have any emails. I was curious as to why these POIs didn't have any emails in the dataset, but I left that question for a different time.

Question 3

I ended up using the algorithm GaussianNB. I tried Decision Tree, SGD, KNeighbors, and SVC. The results using Decision Tree and SGD kept changing everytime I checked the results. KNeighbors and SVC kept warning that "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples." But by using GaussianNB, I was able to get a 93% accuracy, with .75 precision and .5 recall.

Question 4

I used the GaussianNB algorithm which does not accept direct tuning. If I had used SVC I would have been able to tune the C, kernel and gamma. Changing the C parameter controls the balance between correctly classified training points and the smoothness of the decision boundary. Using an rbf or poly kernel would make the SVC use a non-linear hyperplane while a linear kernel uses a linear hyperplane. If you use rbf or poly for the kernel you can also change the gamma which will control the fit of the training dataset.

Question 5

Validation is proving that the algorithm finds the correct answer for all of the inputs that meet the parameters. A classic mistake you can make is testing the algorithm using the same data that you used to train it. I split data so that the test size was 40%. I attempted to validated my analysis by using the tester.py StraitfifedShuffleSplit function. I found the results interesting. While the end results were the same ,DT and SGD flucuated, KN and SVC had zero precision/recall, GaussianNB had actionable results. The scores all changed. Accuracy scores for KN and SVC went from .89 to .86. GaussianNB accuracy droped from .93 to .36. Precision went down from .75 to .18. But recall went up from .5 to 1.0.

Question 6

# DecisionTreeClassifier()              # Accuracy: Changes   Precision: Changes  Recall: Changes
# SGDClassifier()                       # Accuracy: Changes   Precision: Changes  Recall: Changes
# KNeighborsClassifier(n_neighbors=6)   # Accuracy: .89/.86   Precision: .0       Recall: .0
# SVC(C=.001, kernel="rbf", gamma=0.1)  # Accuracy: .89/.86   Precision: .0       Recall: .0
# GaussianNB()                          # Accuracy: .93/.36   Precision: .75/.18  Recall: .5/1.0

As previously stated, the different classifiers provided different bits of infromation. As you can see, the accuracy () for GaussianNB was best while not useing a StraitfifedShuffleSplit. But it always had the best precision (percent of no false positives) and recall (percent of no false negatives), even if that was only because it did not have labels that divided by zero. As the closest to one percent precision and/or recall wins, todays winner is GaussianNB.
